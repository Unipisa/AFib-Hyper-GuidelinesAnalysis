{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from swiplserver import PrologMQI, PrologThread\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from itertools import chain, combinations, product\n",
    "\n",
    "class Target:\n",
    "    def __init__(self,pr,ab):\n",
    "        self.present = pr\n",
    "        self.absent = ab\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"present: {self.present} absent: {self.absent}\"           \n",
    "targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFICATION OF THE PATTERN OF PATIENT PROFILES TO BE ANALYSED\n",
    "# POSSIBLE VALUES FOR EACH FEATURE: \"ANY\",\"YES\",\"NO\"\n",
    "patients_config = {\n",
    "    \"afib\": (\"kafib\",\"YES\"),\n",
    "    \"has_fib\": (\"khf\",\"YES\"),\n",
    "    \"heart_rate\": (\"khr\",\"NO\"),\n",
    "    \"consensus_acei\": (\"kcons\",\"ANY\"),\n",
    "    \"over75\": (\"kageA\",\"ANY\"),  # over75 and below55 will not be generated together (controlled later)\n",
    "    \"below55\": (\"kageB\",\"ANY\"),\n",
    "    \"diabete\": (\"kdiabete\",\"NO\"),\n",
    "    \"doac_int\": (\"kdoacint\",\"NO\"),\n",
    "    \"hyper\": (\"khyper\",\"YES\"),\n",
    "    \"origin\": (\"korigin\",\"NO\")\n",
    "}\n",
    "\n",
    "# CHOOSE ONE OF THE FOLLOWING THREE TARGETS\n",
    "targets.append(Target([\"major\"],[]))\n",
    "#targets.append(Target([\"moderate\"],[\"major\"]))\n",
    "#targets.append(Target([\"minor\"],[\"moderate\",\"major\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SPECIFIC PARAMETERS FOR spec-AFib-hyper\n",
    "patient_params = set([\"afib\",\"has_fib\",\"heart_rate\",\"consensus_acei\",\"over75\",\"below55\",\"diabete\",\"doac_int\",\"hyper\",\"origin\"])\n",
    "adm_drugs = set([\"cbb\",\"diltiazem\",\"verapamil\",\"nsbb\",\"propranolol\",\"carvedilol\",\"sbb\",\"bisoprolol\",\"atenolol\",\"flecainide\",\"warfarin\",\"doac\",\"apixaban\",\"dabigatran\",\"vkant\",\"acei\",\"benazepril\",\"captopril\",\"arb\",\"olmesortan\",\"irbesartan\",\"td\",\"indapamide\",\"chlorothiazide\"])\n",
    "tests_results = set([\"doac_ok\",\"doac_fail\"])\n",
    "CONTEXT_SIZE = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUXILIARY FUNCTIONS\n",
    "def state2context(s):\n",
    "    state = set(s.split(','))\n",
    "    context = \"\"\n",
    "    if \"afib\" in state:\n",
    "        context += \"kafib\"\n",
    "    else:\n",
    "        context += \"empty\"\n",
    "    if \"has_fib\" in state:\n",
    "        context += \",khf\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"heart_rate\" in state:\n",
    "        context += \",khr\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"consensus_acei\" in state:\n",
    "        context += \",kcons\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"over75\" in state:\n",
    "        context += \",kageA\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"below55\" in state:\n",
    "        context += \",kageB\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"diabete\" in state:\n",
    "        context += \",kdiabete\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"doac_int\" in state:\n",
    "        context += \",kdoacint\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"hyper\" in state:\n",
    "        context += \",khyper\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    if \"origin\" in state:\n",
    "        context += \",korigin\"\n",
    "    else:\n",
    "        context += \",empty\"\n",
    "    return context\n",
    "\n",
    "def state2patient(s):\n",
    "    return ';'.join(list(set(re.split(';|,',s)) & patient_params))\n",
    "\n",
    "def state2tests(s):\n",
    "    return ';'.join(list(set(re.split(';|,',s)) & tests_results))\n",
    "\n",
    "def state2patient_with_tests(s):\n",
    "    return ';'.join(list(set(re.split(';|,',s)) & (patient_params | tests_results)))\n",
    "\n",
    "def state2drugs(s):\n",
    "    return ';'.join(list(set(re.split(';|,',s)) & adm_drugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dotfile = nx.nx_pydot.read_dot(\"AFIB3.dot\")\n",
    "#dotfile = nx.nx_pydot.read_dot(\"AFIB-HYP3.dot\")\n",
    "\n",
    "print (\"PATIENTS CONFIGURATION:\")\n",
    "patient_config_list = []\n",
    "for k, v in patients_config.items():\n",
    "    print (v)\n",
    "    if v[1]==\"YES\":\n",
    "        patient_config_list.append([v[0]])\n",
    "    elif v[1]==\"NO\":\n",
    "        patient_config_list.append([\"empty\"])\n",
    "    elif v[1]==\"ANY\":\n",
    "        patient_config_list.append([v[0],\"empty\"])\n",
    "    else:\n",
    "        raise Exception(\"Parse Error\")\n",
    "\n",
    "patients_list = []\n",
    "for specific_patient in product(patient_config_list[0],\n",
    "                                patient_config_list[1],\n",
    "                                patient_config_list[2],\n",
    "                                patient_config_list[3],\n",
    "                                patient_config_list[4],\n",
    "                                patient_config_list[5],\n",
    "                                patient_config_list[6],\n",
    "                                patient_config_list[7],\n",
    "                                patient_config_list[8],\n",
    "                                patient_config_list[9]):\n",
    "    if (\"kageA\" in specific_patient) and (\"kageB\" in specific_patient):\n",
    "        pass\n",
    "    else:\n",
    "        patients_list.append(specific_patient)\n",
    "\n",
    "print()\n",
    "print(\"PATIENTS TO BE ANALYZED: \" + str(len(patients_list)))\n",
    "print()\n",
    "\n",
    "pat_count = 0\n",
    "pat_tot = len(patients_list)\n",
    "G = nx.DiGraph()\n",
    "for specific_patient in patients_list:\n",
    "    pat_count += 1\n",
    "    specific_patient_string = ','.join(specific_patient)\n",
    "    print()\n",
    "    print(\"ANALYZING[\" + str(pat_count) + \"/\" + str(pat_tot) + \"]: \" + specific_patient_string)\n",
    "    param_file = open(\"Bioresolve_guards/params.pl\",'w')\n",
    "    param_file.write('mycontext(\"[eafib1,eafib2,eafib3,ghyper,' + specific_patient_string + ',k_doac]\").\\n\\n')\n",
    "    #eafib1,eafib2,eafib3,ghyper,kafib,khf,khr,kcons,kage,kdiabete,kdoacint,khyper,korigin,k_doac\n",
    "    #param_file.write('mycontext(\"[e12,e13,e14,g12,' + specific_patient_string + ',k_doac]\").\\n\\n')\n",
    "    #param_file.write('mymonitor(\"[ m0 ]\").\\n\\n')\n",
    "    #param_file.write('mymondef(\"[ m0 = ([{' + state + '} inW].no({' + prolog_target +'}) + [-({' + state + '} inW)].m0) ]\").\\n')\n",
    "    param_file.flush()\n",
    "    param_file.close()\n",
    "        \n",
    "    with PrologMQI() as mqi:\n",
    "        with mqi.create_thread() as prolog_thread:\n",
    "            prolog_thread.query('set_prolog_stack(global, limit(16 000 000 000))')\n",
    "            prolog_thread.query('[\"Bioresolve_guards/BioReSolveGuards.pl\"].')\n",
    "            result = prolog_thread.query('main_name(\"dotfile.dot\").')\n",
    "            print(result)\n",
    "            \n",
    "            dotfile = nx.nx_pydot.read_dot(\"dotfile.dot\")\n",
    "            digraph_dotfile = nx.DiGraph(dotfile)\n",
    "            G = nx.compose(digraph_dotfile,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.nx_pydot.write_dot(G,\"AFIB-HYP-generated.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G = nx.DiGraph(dotfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.draw(G, node_size=2, arrowsize=1, width=0.2)\n",
    "#nx.draw_circular(G, node_size=2, arrowsize=1, width=0.2)\n",
    "#nx.draw_kamada_kawai(G, node_size=2, arrowsize=1, width=0.2)\n",
    "#nx.draw_shell(G, node_size=2, arrowsize=1, width=0.2)\n",
    "#nx.draw_kamada_kawai(digraph_dotfile, node_size=2, arrowsize=1, width=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks whether a node is in an attractor\n",
    "def check_node(node):\n",
    "    try:\n",
    "        cycle = list(nx.find_cycle(G,node))\n",
    "        tmp = map(lambda x : x[0]==node or x[1]==node, cycle) \n",
    "        return reduce(lambda b1, b2: b1 or b2, tmp)\n",
    "    except:\n",
    "        return false  # workaround for graphs with deadlock node (should never happen)\n",
    "\n",
    "# compute the list of entities that are present in the attractor reachable form \"node\"\n",
    "def compute_attractor(node):\n",
    "    cycle = list(nx.find_cycle(G,node))\n",
    "    tmp1 = map(lambda x: x[0].split(';') + x[1].split(';'), cycle)\n",
    "    tmp2 = reduce(lambda x, y: x+y,tmp1)\n",
    "    res = list(dict.fromkeys(tmp2))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def count_states (d):\n",
    "    tmp = 0\n",
    "    for v in d.values():\n",
    "        tmp += len(v)\n",
    "    return tmp\n",
    "\n",
    "# finds computations (LTS traces) that lead to the \"target\"\n",
    "def target_computations(G,target):\n",
    "    all_nodes = list(G.nodes)\n",
    "    filtered = [k for k in all_nodes if check_node(k)] # filters out intermediate nodes (not in attractor)\n",
    "    attractors_map = dict()\n",
    "    for f in filtered:\n",
    "        attractors_map[f] = compute_attractor(f) # creates a map \"state -> attractor\"\n",
    "    for s in target.present:\n",
    "        filtered = [k for k in filtered if s in attractors_map[k]]   # filters out states that do not contain s (present in the target)\n",
    "    for s in target.absent:\n",
    "        filtered = [k for k in filtered if s not in attractors_map[k]] # filters out state that contain s (absent in the target)\n",
    "    \n",
    "    # filters out states in target attractors, but that do not contain any entity in target.present\n",
    "    # (slicing analysis - not yet implemented - would give an empty result on these states)\n",
    "    # NOTE: this only if target.present is not empty, otherwise filtered2 would be empty as well\n",
    "    if (len(target.present)>0):\n",
    "        filtered2 = [k for k in filtered if len(list(set(k.split(';')) & set(target.present)))>0]\n",
    "    else:\n",
    "        filtered2 = filtered\n",
    "    \n",
    "    # cleans the attractors_map from states not in target (this step is not really necessary...)\n",
    "    keys_to_delete = list() \n",
    "    for k in attractors_map.keys():\n",
    "        if k not in filtered: keys_to_delete.append(k) \n",
    "    for k in keys_to_delete:\n",
    "        del attractors_map[k] \n",
    "    \n",
    "    patients = list()               # list of the patients that lead to the target\n",
    "    patients_with_tests = list()    # list of the patients that lead to the target distinguished also by tests results\n",
    "    patients_dict = {}              # for each patient, lists the states in the corresponding attractor\n",
    "    \n",
    "    for f in filtered2:\n",
    "        next_patient = state2patient(f).split(';')\n",
    "        next_patient_with_tests = state2patient_with_tests(f).split(';')\n",
    "        pure_state = (f.split(';'))[CONTEXT_SIZE:] # the first CONTEXT_SIZE entities are the context\n",
    "        if (not next_patient in patients):\n",
    "            patients.append(next_patient)\n",
    "        if (not next_patient_with_tests in patients_with_tests):\n",
    "            patients_with_tests.append(next_patient_with_tests)\n",
    "            patients_dict[','.join(next_patient_with_tests)] = [','.join(pure_state)]\n",
    "        else:\n",
    "            patients_dict[','.join(next_patient_with_tests)].append(','.join(pure_state))\n",
    "    print(\"TARGET --> \" + str(target))\n",
    "    print(\"found \" + str(len(patients)) + \" patients that lead to the target\")\n",
    "    print(\"found \" + str(len(patients_with_tests)) + \" patients (distinguished also by tests results) that lead to the target\")\n",
    "    print(\"found \" + str(count_states(patients_dict) + (len(filtered)-len(filtered2))) + \" states in reachable attractors\")\n",
    "    print(\"of which \" + str(count_states(patients_dict)) + \" with entities in the target\")\n",
    "    print(\"NOTE: at the moment I can't distinguish between n different attractors for the same patient,\")\n",
    "    print(\"      and a single attractor of size n\")\n",
    "    print()\n",
    "    return patients, patients_with_tests, patients_dict       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FILE_OUTPUT = False\n",
    "\n",
    "target_tot = len(targets)\n",
    "target_count = 0\n",
    "\n",
    "if FILE_OUTPUT:\n",
    "    print(\"Writing to file...: \" + \"attractors_for_\" + str(target.present) + \"--\" + str(target.absent) + \".txt\")\n",
    "    output_file = open(\"attractors_for_\" + str(target.present) + \"--\" + str(target.absent) + \".txt\",\"w\")\n",
    "    output_file.write(\"TO BE ANALYZED: \" + str(target_tot) + \" target \\n \\n\")\n",
    "else:\n",
    "    print(\"TO BE ANALYZED: \" + str(target_tot) + \" target\")\n",
    "    print()\n",
    "\n",
    "for target in targets:\n",
    "    target_count = target_count+1\n",
    "    if FILE_OUTPUT:\n",
    "        output_file.write(\"TARGET COUNT: \" + str(target_count) + \"/\" + str(target_tot) + \"\\n\")\n",
    "    else:\n",
    "        print(\"TARGET COUNT: \" + str(target_count) + \"/\" + str(target_tot))\n",
    "        \n",
    "    patients, patients_with_tests, patients_dict = target_computations(G,target)\n",
    "\n",
    "    prolog_target = ','.join(target.present)\n",
    "\n",
    "    cont = 1\n",
    "    tot = str(count_states(patients_dict))\n",
    "    union_set = set()\n",
    "    intersection_set = set()\n",
    "    first_time = True\n",
    "    for ptn in patients_with_tests:\n",
    "        prolog_context = state2context(','.join(ptn))\n",
    "        prolog_target_states = patients_dict[','.join(ptn)]\n",
    "        for i,state in enumerate(prolog_target_states):\n",
    "            if FILE_OUTPUT:\n",
    "                output_file.write(\"TEST CASE: \" + str(cont) + \"/\" + str(tot) + \"\\n\")\n",
    "                cont=cont+1\n",
    "                output_file.write(\"PATIENT: \" + state2patient(state) + \"      ATTRACTOR STATE: \" + str(i+1) + \"/\" + str(len(prolog_target_states))  + \"\\n\")\n",
    "                output_file.write(\"DRUG TESTS: \" + state2tests(state) + \"\\n\")\n",
    "                output_file.write(\"CONTEXT: \" + state2context(state) + \"\\n\")\n",
    "                output_file.write(\"ADMINISTERED DRUGS: \" + state2drugs(state) + \"\\n\")\n",
    "                output_file.write(\"WHOLE STATE: \" + state + \"\\n \\n\")\n",
    "            else:\n",
    "                print(\"TEST CASE: \" + str(cont) + \"/\" + str(tot))\n",
    "                cont=cont+1\n",
    "                print(\"PATIENT: \" + state2patient(state) + \"      ATTRACTOR STATE: \" + str(i+1) + \"/\" + str(len(prolog_target_states)))\n",
    "                print(\"DRUG TESTS: \" + state2tests(state))\n",
    "                print(\"CONTEXT: \" + state2context(state))\n",
    "                print(\"ADMINISTERED DRUGS: \" + state2drugs(state))\n",
    "                print(\"WHOLE STATE: \" + state)\n",
    "                print()\n",
    "\n",
    "if FILE_OUTPUT:\n",
    "    output_file.close()\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logicmin\n",
    "\n",
    "def append_test(ctx,lis):\n",
    "    tr = list(tests_results)\n",
    "    if tr[0] in lis:\n",
    "        ctx.append(tr[0])\n",
    "    else:\n",
    "        ctx.append(\"empty\")\n",
    "    if tr[1] in lis:\n",
    "        ctx.append(tr[1])\n",
    "    else:\n",
    "        ctx.append(\"empty\")\n",
    "    # ctx += list(set(lis) & tests_results)\n",
    "    return ctx\n",
    "\n",
    "for target in targets:\n",
    "    patients, patients_with_tests, patients_dict = target_computations(G,target)\n",
    "    if (len(patients)>0):\n",
    "        contexts = list(map(lambda lis: append_test(state2context(','.join(lis)).split(','),lis),patients_with_tests))\n",
    "        contexts2 = list(map(lambda lis: list(map(lambda x: 0 if x=='empty' else 1,lis)),contexts))    \n",
    "        df = pd.DataFrame(contexts2)\n",
    "        df.columns = [\"afib\",\"has_fib\",\"heart_rate\",\"consensus_acei\",\"over75\",\"below55\",\"diabete\",\"doac_int\",\"hyper\",\"origin\",\"doac_fail\",\"doac_ok\"]\n",
    "        #df = tmp.sort_values(by=[5,4,3,2,1,0],ignore_index=True)\n",
    "        print(\"CONTEXTS THAT LEAD TO THE TARGET:\")\n",
    "        print(df)\n",
    "        df.to_csv(\"contexts_to_\" + str(target.present) + \"--\" + str(target.absent) + \".csv\",index=False,header=True)\n",
    "        \n",
    "        print()\n",
    "        # truth table 3 inputs, 2 outputs\n",
    "        t = logicmin.TT(len(df.columns),1);\n",
    "        for row in contexts2:\n",
    "            t.add(''.join(list(map(lambda i:str(i),row))),\"1\")\n",
    "        sols = t.solve()\n",
    "        print(sols.printN(xnames=df.columns,ynames=['target'], info=True, syntax=\"VHDL\"))\n",
    "        logic_file = open(\"logic_formula_for_\" + str(target.present) + \"--\" + str(target.absent) + \".txt\",\"w\")\n",
    "        logic_file.write(sols.printN(xnames=df.columns,ynames=['target'], info=True, syntax=\"VHDL\"))\n",
    "        logic_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
